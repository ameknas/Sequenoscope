{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ameknas\\Desktop\\sequenoscope-1\\Sequenoscope\\analyze\\parser_analysis.ipynb Cell 1\u001b[0m in \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ameknas/Desktop/sequenoscope-1/Sequenoscope/analyze/parser_analysis.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ameknas/Desktop/sequenoscope-1/Sequenoscope/analyze/parser_analysis.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39margparse\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39map\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ameknas/Desktop/sequenoscope-1/Sequenoscope/analyze/parser_analysis.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparser\u001b[39;00m \u001b[39mimport\u001b[39;00m GeneralSeqParser\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import argparse as ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('histogram_file.dist_analysis.json'))\n",
    "\n",
    "df = pd.DataFrame(data[\"peaks\"])\n",
    "global_maxima = data[\"global_maxima\"]\n",
    "print(df)\n",
    "print(\"-\"*80)\n",
    "print(global_maxima)\n",
    "print(\"-\"*80)\n",
    "value = data[\"mean_freq\"]\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('output-stats.tsv', sep='\\t', header=0, index_col=0)\n",
    "data\n",
    "# data_info = data[[\"invalid_kmers\", \"non_zero_kmers\", \"seq_length\", \"%_non_zero\"]]\n",
    "# data_info\n",
    "# data_info[[\"non_zero_kmers\"]]\n",
    "\n",
    "# value = data[\"non_zero_kmers\"].sum()\n",
    "# print(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('test_sequences/sequencing_summary_FAT53867_9a53b23a.txt', sep='\\t', index_col=0)\n",
    "\n",
    "#general data check\n",
    "print(data.describe())\n",
    "print(\"-\"*40)\n",
    "\n",
    "\n",
    "##check read_id duplicates\n",
    "df1 = len(data[\"read_id\"])\n",
    "df2 = len(data[\"read_id\"].drop_duplicates())\n",
    "if df1 == df2:\n",
    "    print(\"there really aren't any duplicated\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "big pp\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('test_sequences/sequencing_summary_FAT53867_9a53b23a.txt', delimiter='\\t')\n",
    "select_data = data[[\"read_id\", \"channel\", \"start_time\", \"duration\", \"sequence_length_template\", \"mean_qscore_template\",\n",
    "                    \"end_reason\"]]\n",
    "select_data.reset_index(drop=True, inplace=True)\n",
    "select_data\n",
    "list_of_headers = [\"read_id\", \"channel\", \"start_time\", \"duration\", \"sequence_length_template\", \"mean_qscore_template\", \"end_reason\"]\n",
    "\n",
    "if not set(list_of_headers).issubset(set(df.columns)):\n",
    "    raise ValueError(\"Error: column headers did not match expected output. check file.\")\n",
    "else:\n",
    "    print(\"big pp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>read_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23091</th>\n",
       "      <td>a0c5c319-be6f-4d68-84d4-d0ef6ff7dba9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83993</th>\n",
       "      <td>ca36a63c-e7f2-4e92-8eab-3634a8e2e2b0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174987</th>\n",
       "      <td>c1982d7a-2c1c-43cd-8a39-75453c632d65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322496</th>\n",
       "      <td>b1950e6a-e609-4be0-8fed-218a38c70161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     read_id\n",
       "23091   a0c5c319-be6f-4d68-84d4-d0ef6ff7dba9\n",
       "83993   ca36a63c-e7f2-4e92-8eab-3634a8e2e2b0\n",
       "174987  c1982d7a-2c1c-43cd-8a39-75453c632d65\n",
       "322496  b1950e6a-e609-4be0-8fed-218a38c70161"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification = ['signal_positive',\n",
    " 'data_service_unblock_mux_change',\n",
    " 'signal_negative',\n",
    " 'unblock_mux_change']\n",
    "min_ch = 1\n",
    "max_ch = 1\n",
    "min_dur = 1\n",
    "max_dur = max(select_data.duration)\n",
    "min_q = 5\n",
    "max_q = 6\n",
    "min_len = 500\n",
    "max_len = 5000\n",
    "\n",
    "filtered_reads = select_data[(select_data[\"end_reason\"].isin(classification)) & \n",
    "                             (select_data.channel.between(min_ch, max_ch)) & \n",
    "                             (select_data.duration.between(min_dur, max_dur)) &\n",
    "                             (select_data.mean_qscore_template.between(min_q, max_q)) & \n",
    "                             (select_data.sequence_length_template.between(min_len, max_len))]\n",
    "\n",
    "\n",
    "filtered_reads[[\"read_id\"]]\n",
    "# filtered_reads[[\"read_ids\"]].to_csv(\"filtered_reads.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ont_summary_file(file_path):\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(\"Error: file not found\")\n",
    "        return False\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    if not lines or len(lines) < 2:\n",
    "        print(\"Error: file is empty or has fewer than 2 lines\")\n",
    "        return False\n",
    "    \n",
    "    for line in lines[1:]:\n",
    "        fields = line.strip().split(\"\\t\")\n",
    "        if len(fields) != 41:\n",
    "            print(f\"Error: malformed line: {line.strip()}\")\n",
    "            return False\n",
    "        try:\n",
    "            int(fields[4])\n",
    "        except ValueError:\n",
    "            print(f\"Error: invalid channel designation: {fields[4]}\")\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "check_ont_summary_file(\"test_sequences/sequencing_summary_FAT53867_9a53b23a.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\"stop_receiving\":[\"signal_positive\"], \"unblocked\":[\"data_service_unblock_mux_change\"],\n",
    "               \"no_decision\":[\"signal_negative\", \"unblock_mux_change\"]}\n",
    "\n",
    "classification = \"no_decision\"\n",
    "\n",
    "new_classification = classes[classification]\n",
    "pp = list(classes.values())\n",
    "ppp = []\n",
    "for x in pp:\n",
    "    ppp.extend(x)\n",
    "ppp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_sequences/Test_br1_sal_lam_enriched.fastq\", \"r\") as f:\n",
    "    first_line = f.readline().strip()\n",
    "    second_line = f.readline().strip()\n",
    "    third_line = f.readline().strip()\n",
    "    fourth_line = f.readline().strip()\n",
    "\n",
    "if len(second_line) == len(fourth_line):\n",
    "    print(\"yeahh buddy, light weight baby\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/mnt/c/Users/ameknas/Desktop/Sequenoscope/Sequenoscope/sequenoscope/analyze/test_sequences/Test_br1_sal_lam_enriched.fastq']\n"
     ]
    }
   ],
   "source": [
    "parser = ap.ArgumentParser(prog=\"sequenoscope\", usage=\"%(prog)s <command> <argument>\", description=\"%(prog)s version 1.0: a tool for analyzing and processing sequencing data.\",\n",
    "                           formatter_class= ap.RawTextHelpFormatter)\n",
    "\n",
    "parser._positionals.title = \"Commands\"\n",
    "# parser._positionals\n",
    "parser._optionals.title = \"Help\"\n",
    "# parser.add_argument('--foo', action='store_true', help='foo help')\n",
    "subparsers = parser.add_subparsers()\n",
    "\n",
    "analyze_parser = subparsers.add_parser('analyze', help='map reads to a target and produce a report with sequencing statistics')\n",
    "plot_parser = subparsers.add_parser('plot', help='generate plots based on fastq or kmer hash files')\n",
    "\n",
    "\n",
    "filter_parser = subparsers.add_parser('filter_ONT', help='filter reads from a fastq file based on a sequencing summary file')\n",
    "filter_parser.usage = \"sequenoscope filter_ONT --input_fastq <file.fq> --input_summary <seq_summary.txt> -o <out.fastq> [options]\"\n",
    "filter_parser.add_argument(\"--input_fastq\", metavar=\"\", required=True, nargs=\"+\", help=\"[REQUIRED] Path to adaptive sequencing fastq files to process.\")\n",
    "filter_parser.add_argument(\"--input_summary\", metavar=\"\", required=True, help=\"[REQUIRED] Path to ONT sequencing summary file.\")\n",
    "filter_parser.add_argument(\"-o\", \"--output\", metavar=\"\", required=True, help=\"[REQUIRED] Output directory/file designation\")\n",
    "filter_parser.add_argument(\"-cls\", \"--classification\", metavar=\"\", choices=['unblocked', 'stop_receiving', 'no_decision'], help=\"a designation of the adaptive-sampling sequencing decision classification ['unblocked', 'stop_receiving', or 'no_decision']\")\n",
    "filter_parser.add_argument(\"-min_ch\", \"--minimum_channel\", default=1, metavar=\"\", type=int, help=\"a designation of the minimum channel/pore number for filtering reads\")\n",
    "filter_parser.add_argument(\"-max_ch\", \"--maximum_channel\", default=512, metavar=\"\", type=int, help=\"a designation of the maximum channel/pore number for filtering reads\")\n",
    "filter_parser.add_argument(\"-min_dur\", \"--minimum_duration\", metavar=\"\", type=float, help=\"a designation of the minimum duration of the sequencing run in SECONDS for filtering reads\")\n",
    "filter_parser.add_argument(\"-max_dur\", \"--maximum_duration\", metavar=\"\", type=float, help=\"a designation of the maximum duration of the sequencing run in SECONDS for filtering reads\")\n",
    "filter_parser.add_argument(\"-min_start\", \"--minimum_start_time\", metavar=\"\", type=float, help=\"a designation of the minimum start time of the sequencing run in SECONDS for filtering reads\")\n",
    "filter_parser.add_argument(\"-max_start\", \"--maximum_start_time\", metavar=\"\", type=float, help=\"a designation of the maximum start time of the sequencing run in SECONDS for filtering reads\")\n",
    "filter_parser.add_argument(\"-min_q\", \"--minimum_q_score\", metavar=\"\", type=int, help=\"a designation of the minimum q score for filtering reads\")\n",
    "filter_parser.add_argument(\"-max_q\", \"--maximum_q_score\", metavar=\"\", type=int, help=\"a designation of the maximum q score for filtering reads\")\n",
    "filter_parser.add_argument(\"-min_len\", \"--minimum_length\", metavar=\"\", type=int, help=\"a designation of the minimum read length for filtering reads\")\n",
    "filter_parser.add_argument(\"-max_len\", \"--maximum_length\", metavar=\"\", type=int, help=\"a designation of the maximum read length for filtering reads\")\n",
    "# parser.print_help()\n",
    "args = parser.parse_args([\"filter_ONT\", \"--input_fastq\", \"/mnt/c/Users/ameknas/Desktop/Sequenoscope/Sequenoscope/sequenoscope/analyze/test_sequences/Test_br1_sal_lam_enriched.fastq\", \"--input_summary\", \"<seq_summary.txt>\", \"-o\", \"<out.fastq>\", \"-cls\", \"no_decision\"])\n",
    "# filter_parser.print_help()\n",
    "\n",
    "clas = args.input_fastq\n",
    "print(clas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kkk'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SeqSummaryProcesser:\n",
    "    parsed_report_object = None\n",
    "    out_dir = None\n",
    "    out_prefix = None\n",
    "    classification = None\n",
    "    min_ch = None\n",
    "    max_ch = None\n",
    "    min_dur = None\n",
    "    max_dur = None\n",
    "    min_start_time = None\n",
    "    max_start_time = None\n",
    "    min_q = None\n",
    "    max_q = None\n",
    "    min_len = None\n",
    "    max_len = None\n",
    "    status = False\n",
    "    status_read_id = False\n",
    "    error_messages = None\n",
    "    result_files = {\"filtered_read_id_list\":\"\"}\n",
    "    classes = {\"stop_receiving\":[\"signal_positive\"], \"unblocked\":[\"data_service_unblock_mux_change\"],\n",
    "               \"no_decision\":[\"signal_negative\", \"unblock_mux_change\"], \"all\":[\"signal_positive\", \"data_service_unblock_mux_change\", \"signal_negative\", \"unblock_mux_change\"]}\n",
    "\n",
    "    def __init__(self, parsed_report_object, out_dir, out_prefix, classification=\"all\", min_ch=0, max_ch=512, min_dur=0, max_dur=None,\n",
    "                 min_start_time=0, max_start_time=None, min_q=0, max_q=None, min_len=0, max_len=None ):\n",
    "        \"\"\"\n",
    "        Initalize the class with parsed_report_object, out_dir, and out_prefix\n",
    "\n",
    "        Arguments:\n",
    "            parsed_report_object: parser object\n",
    "                an object that contains the parsed sequencing summary report for analysis\n",
    "            out_prefix: str\n",
    "                a designation of what the output files will be named\n",
    "            out_dir: str\n",
    "                a designation of what the output files will be stored\n",
    "            classification: str\n",
    "                a designation of the adaptive sampling classification based on the sequencing \n",
    "                summary file generated by ONT\n",
    "            min_ch: integer\n",
    "                a designation that indicates the minimum channel number in an ONT run\n",
    "            max_ch: integer\n",
    "                a designation that indicates the maximum channel number in an ONT run\n",
    "            min_dur: integer\n",
    "                a designation that indicates the minimum duration a read has undergone in an ONT run\n",
    "            max_dur: integer\n",
    "                a designation that indicates the maximum duration a read has undergone in an ONT run\n",
    "            min_start_time: integer\n",
    "                a designation that indicates the minimum start time in seconds for reads\n",
    "            max_start_time: integer\n",
    "                a designation that indicates the maximum start time in seconds for reads\n",
    "            min_q = integer\n",
    "                a designation that indicates the minimum q-score of reads in an ONT run\n",
    "            max_q = integer\n",
    "                a designation that indicates the maximum q-score of reads in an ONT run\n",
    "            min_len = len\n",
    "                a designation that indicates the minimum length of a sequence for the reads in an ONT run\n",
    "            max_len = len\n",
    "                a designation that indicates the maximum length of a sequence for the reads in an ONT run\n",
    "        \"\"\"\n",
    "        self.parsed_report_object = parsed_report_object.parsed_file\n",
    "        self.out_dir = out_dir\n",
    "        self.out_prefix = out_prefix\n",
    "        self.classification = self.classes[classification]\n",
    "        self.min_ch = min_ch\n",
    "        self.max_ch = max_ch\n",
    "        self.min_dur = min_dur\n",
    "        self.max_dur = max_dur or max(self.parsed_report_object.duration)\n",
    "        self.min_start_time = min_start_time\n",
    "        self.max_start_time = max_start_time or max(self.parsed_report_object.start_time)\n",
    "        self.min_q = min_q\n",
    "        self.max_q = max_q or max(self.parsed_report_object.mean_qscore_template)\n",
    "        self.min_len = min_len\n",
    "        self.max_len = max_len or max(self.parsed_report_object.sequence_length_template)\n",
    "        pass\n",
    "\n",
    "    def generate_read_ids(self):\n",
    "\n",
    "        read_id_list = os.path.join(self.out_dir,\"{}.csv\".format(self.out_prefix))\n",
    "\n",
    "        self.result_files[\"filtered_read_id_list\"] = read_id_list\n",
    "\n",
    "        filtered_reads = self.parsed_report_object[(self.parsed_report_object[\"end_reason\"].isin(self.classification)) & \n",
    "                             (self.parsed_report_object.channel.between(self.min_ch, self.max_ch)) & \n",
    "                             (self.parsed_report_object.duration.between(self.min_dur, self.max_dur)) &\n",
    "                             (self.parsed_report_object.start_time.between(self.min_start_time, self.max_start_time)) &\n",
    "                             (self.parsed_report_object.mean_qscore_template.between(self.min_q, self.max_q)) & \n",
    "                             (self.parsed_report_object.sequence_length_template.between(self.min_len, self.max_len))]\n",
    "        \n",
    "        filtered_reads[[\"read_id\"]].to_csv(read_id_list, index=False)\n",
    "\n",
    "        self.status = self.check_files([read_id_list])\n",
    "        if self.status == False:\n",
    "            self.error_messages = \"one or more files was not created or was empty\"\n",
    "            raise ValueError(str(self.error_messages))\n",
    "        else:\n",
    "            self.status_read_id = self.check_read_id_file(read_id_list)\n",
    "            if self.status_read_id == False:\n",
    "                self.error_messages = \"File has less than 2 lines. No reads that match filtering criteria\"\n",
    "                raise ValueError(str(self.error_messages))\n",
    "        pass\n",
    "\n",
    "    def check_files(self, files_to_check):\n",
    "        \"\"\"\n",
    "        check if the output file exists and is not empty\n",
    "\n",
    "        Arguments:\n",
    "            files_to_check: list\n",
    "                list of file paths\n",
    "\n",
    "        Returns:\n",
    "            bool:\n",
    "                returns True if the generated output file is found and not empty, False otherwise\n",
    "        \"\"\"\n",
    "        if isinstance (files_to_check, str):\n",
    "            files_to_check = [files_to_check]\n",
    "        for f in files_to_check:\n",
    "            if not os.path.isfile(f):\n",
    "                return False\n",
    "            elif os.path.getsize(f) == 0:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def check_read_id_file(self, file):\n",
    "        \"\"\"\n",
    "        check if the read id file has more than 1 line\n",
    "\n",
    "        Arguments:\n",
    "            file: str\n",
    "                designation of read_id file path\n",
    "\n",
    "        Returns:\n",
    "            bool:\n",
    "                returns True if the generated read_id file has greater than 2 lines, False otherwise\n",
    "        \"\"\"\n",
    "        with open(file, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        if not lines or len(lines) < 2:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "\n",
    "class seq_summarry_process:\n",
    "    result_files = {\"filtered_read_id_list\":\"kkk\"}\n",
    "\n",
    "jjj = seq_summarry_process()\n",
    "\n",
    "jjj.result_files[\"filtered_read_id_list\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4006e8cb3b36a78823c465790770e1ecf556d7c18fd72bd451abd8d142aa69db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
